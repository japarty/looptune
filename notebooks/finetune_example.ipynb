{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e03f32711a005f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune_new_test\\Lib\\site-packages\\IPython\\utils\\_process_win32.py:124: ResourceWarning: unclosed file <_io.BufferedWriter name=3>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune_new_test\\Lib\\site-packages\\IPython\\utils\\_process_win32.py:124: ResourceWarning: unclosed file <_io.BufferedReader name=4>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune_new_test\\Lib\\site-packages\\IPython\\utils\\_process_win32.py:124: ResourceWarning: unclosed file <_io.BufferedReader name=5>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -e ../.\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from looptune import prep_config_combinations, single_run, clean_memory\n",
    "from transformers import Phi3ForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269d739-e320-4d04-aae1-466d6e1f3cbb",
   "metadata": {},
   "source": [
    "### Prepare dataset with two columns: 'text' and 'label'\n",
    "\n",
    "Examplary data source: https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca218249-1791-49ec-a1be-fe8927c744e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('example_data/SentimentAnalysisforFinancialNews.csv', encoding=\"ISO-8859-1\", header=None)\n",
    "df.columns = ['label', 'text']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e488de-cfd6-42d1-9280-5823b4394805",
   "metadata": {},
   "source": [
    "### Prepare run configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca629d5cfeeb48dc",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'model_name': 'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
       "  'split': (0.8, 0.2),\n",
       "  'balanced': (('train',), ('test',)),\n",
       "  'training_arguments': {'num_train_epochs': 10,\n",
       "   'per_device_train_batch_size': 16,\n",
       "   'per_device_eval_batch_size': 16,\n",
       "   'save_total_limit': 2,\n",
       "   'load_best_model_at_end': True,\n",
       "   'save_strategy': 'steps',\n",
       "   'save_steps': 100,\n",
       "   'metric_for_best_model': 'eval_loss',\n",
       "   'evaluation_strategy': 'steps',\n",
       "   'logging_steps': 100,\n",
       "   'fp16': False,\n",
       "   'learning_rate': 5e-05,\n",
       "   'lr_scheduler_type': 'linear',\n",
       "   'warmup_ratio': 0.1,\n",
       "   'max_grad_norm': 0.3,\n",
       "   'weight_decay': 0.001},\n",
       "  'trainer': {'callbacks': <transformers.trainer_callback.EarlyStoppingCallback at 0x207a12ea350>},\n",
       "  'bnb_config': False,\n",
       "  'peft_config': False}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Phi3ForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "run_config = {   # -----------------------\n",
    "                  'model_name': [\n",
    "                            'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n",
    "                           #  'michellejieli/emotion_text_classifier',\n",
    "                           #  'cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
    "                           #  'celine98/canine-s-finetuned-sst2',\n",
    "                           #  'lxyuan/distilbert-base-multilingual-cased-sentiments-student',\n",
    "                           #  'michelecafagna26/t5-base-finetuned-sst2-sentiment',\n",
    "                           # 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "                           # 'ProsusAI/finbert',\n",
    "                           # 'arpanghoshal/EmoRoBERTa',\n",
    "                           # 'camembert-base'\n",
    "                           # 'cardiffnlp/twitter-roberta-base-irony',\n",
    "                           # 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "                           # 'ctrl',\n",
    "                           # 'distilroberta-base',\n",
    "                           # 'flaubert/flaubert_base_cased',\n",
    "                           # 'j-hartmann/emotion-english-distilroberta-base',\n",
    "                           # 'joeddav/distilbert-base-uncased-go-emotions-student',\n",
    "                           # 'lxyuan/distilbert-base-multilingual-cased-sentiments-student',\n",
    "                           # 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "                           # 'papluca/xlm-roberta-base-language-detection',\n",
    "                           # 'roberta-base',\n",
    "                           # 'xlnet-base-cased',\n",
    "                           # 'facebook/tart-full-flan-t5-xl',\n",
    "                           # 'lytang/MiniCheck-Flan-T5-Large',\n",
    "                          # Need peft on 10GB GPU\n",
    "                          #  'microsoft/Phi-3-mini-4k-instruct',\n",
    "                           # 'microsoft/phi-2',\n",
    "                          # 'meta-llama/Meta-Llama-3-8B',\n",
    "                           #'lightblue/suzume-llama-3-8B-multilingual',\n",
    "                           # 'google/gemma-2b',\n",
    "                           # 'mistralai/Mistral-7B-v0.1',\n",
    "                           #'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "                           # 'tiiuae/falcon-11B' ,\n",
    "                           ], # Pre-trained model names from the Hugging Face hub used for fine-tuning\n",
    "                    #'custom_loader': Phi3ForSequenceClassification,\n",
    "                 # --------------------------\n",
    "                 'split': (0.8, 0.2), # Divides the dataset into training, testing, (and optionally) validation sets. Examples: (0.7,0.3) -> split into train and test proportionally; (70, 30) splits into train,test proportionally.\n",
    "                 'balanced': (('train',), ('test',)),\n",
    "                 # --------------------------\n",
    "                 'training_arguments': {\n",
    "                     'num_train_epochs': 10, # Number of times the model sees the entire training dataset.\n",
    "                     'per_device_train_batch_size': 16, # int: Number of samples processed in each training step (personally, 8/16 work best, 16 is faster, but you may find linear drop in inference speed during fine-tuning).\n",
    "                     'per_device_eval_batch_size': 16, # int:  Number of samples processed in each evaluation step.\n",
    "                     #'gradient_accumulation_steps': 4,\n",
    "                     #'gradient_checkpointing': True,\n",
    "                     #-----------------------------\n",
    "                     'save_total_limit': 2,\n",
    "                     'load_best_model_at_end': True,\n",
    "                     'save_strategy': 'steps', # ('steps', 'epoch' or 'no').\n",
    "                     'save_steps': 100,\n",
    "                     'metric_for_best_model': 'eval_loss',\n",
    "                     #-----------------------------\n",
    "                     'evaluation_strategy': \"steps\", # ('steps', 'epoch')\n",
    "                     'logging_steps': 100, # int\n",
    "                     #'max_steps': 100, # int\n",
    "                     'fp16': False, # bool\n",
    "                     # 'use_cpu': False,\n",
    "                     #-----------------------------\n",
    "                     'learning_rate': 5e-5,\n",
    "                     'lr_scheduler_type': \"linear\",\n",
    "                     'warmup_ratio': 0.1,\n",
    "                     'max_grad_norm': 0.3,\n",
    "                     'weight_decay': 0.001,\n",
    "                 },\n",
    "                 #-----------------------------\n",
    "                    'trainer': {'callbacks': EarlyStoppingCallback(early_stopping_patience=2)},\n",
    "                     'bnb_config': [\n",
    "                                 False,\n",
    "                                #{'bnb_4bit_compute_dtype': torch.bfloat16, 'load_in_4bit': True, 'bnb_4bit_quant_type': \"nf4\", 'bnb_4bit_use_double_quant': True, 'load_in_8bit': False}\n",
    "                                 ],\n",
    "                 'peft_config': [\n",
    "                                False,\n",
    "                                #{'r': 64, 'lora_alpha': 32, 'lora_dropout': 0.1, 'bias': \"none\",\n",
    "                                #'task_type': \"SEQ_CLS\", \n",
    "                                #'target_modules': \"all-linear\",}\n",
    "                                ],\n",
    "                    }\n",
    "\n",
    "run_params_serie = prep_config_combinations(run_config)\n",
    "print(len(run_params_serie))\n",
    "run_params_serie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc109c-814b-4b32-9d5e-6951e2b746b4",
   "metadata": {},
   "source": [
    "### Run in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10da002dd892e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          text\n",
      "label         \n",
      "negative   604\n",
      "neutral   2879\n",
      "positive  1363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba919cc81354666ab0234e7eb3eca44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb1e388e0774f76a92d75f2738968fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7e9cfc50af439ea56250f07bf39a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/363 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c7611c574045f98fc1a780e0566cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff44df771b9941d79a5ecb9355349be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/363 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'callbacks': <transformers.trainer_callback.EarlyStoppingCallback object at 0x00000207A12EA350>}\n",
      "GPU = NVIDIA GeForce RTX 3080. Max memory = 10.0 GB.\n",
      "3.506 GB of memory reserved.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/910 00:21 < 00:43, 14.00 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.760600</td>\n",
       "      <td>0.383754</td>\n",
       "      <td>0.862960</td>\n",
       "      <td>0.862259</td>\n",
       "      <td>0.861653</td>\n",
       "      <td>0.862259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.320600</td>\n",
       "      <td>0.427031</td>\n",
       "      <td>0.874513</td>\n",
       "      <td>0.873278</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.873278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>0.710252</td>\n",
       "      <td>0.860433</td>\n",
       "      <td>0.856749</td>\n",
       "      <td>0.854299</td>\n",
       "      <td>0.856749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.5087 seconds used for training.\n",
      "0.36 minutes used for training.\n",
      "Peak reserved memory = 3.506 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 35.06 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for run_params in run_params_serie:\n",
    "    trainer = single_run(run_params, df, ['trainer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
