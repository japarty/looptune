{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e03f32711a005f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd5afa0-ff1a-4f7d-bcf3-7505af1b88b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96971ed2-e497-43c4-9a22-47ff7d93c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install proper version of torch, as according to: https://pytorch.org/\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from looptune import prep_config_combinations, single_run, clean_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269d739-e320-4d04-aae1-466d6e1f3cbb",
   "metadata": {},
   "source": [
    "### Prepare dataset with two columns: 'text' and 'label'\n",
    "\n",
    "Examplary data source: https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca218249-1791-49ec-a1be-fe8927c744e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('example_data/SentimentAnalysisforFinancialNews.csv', encoding=\"ISO-8859-1\", header=None)\n",
    "df.columns = ['label', 'text']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e488de-cfd6-42d1-9280-5823b4394805",
   "metadata": {},
   "source": [
    "### Prepare run configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca629d5cfeeb48dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_name': 'meta-llama/Meta-Llama-3-8B',\n",
       "  'split': (0.7, 0.3),\n",
       "  'binary': False,\n",
       "  'balanced': (('train',), ('test',)),\n",
       "  'training_arguments': {'num_train_epochs': 1,\n",
       "   'per_device_train_batch_size': 4,\n",
       "   'per_device_eval_batch_size': 4,\n",
       "   'gradient_checkpointing': True,\n",
       "   'save_total_limit': 2,\n",
       "   'load_best_model_at_end': True,\n",
       "   'save_strategy': 'steps',\n",
       "   'metric_for_best_model': 'f1-score',\n",
       "   'evaluation_strategy': 'steps',\n",
       "   'logging_steps': 100,\n",
       "   'fp16': False,\n",
       "   'learning_rate': 2e-05,\n",
       "   'lr_scheduler_type': 'linear',\n",
       "   'warmup_ratio': 0.1,\n",
       "   'max_grad_norm': 0.3,\n",
       "   'weight_decay': 0.001},\n",
       "  'bnb_config': {'bnb_4bit_compute_dtype': torch.bfloat16,\n",
       "   'load_in_4bit': True,\n",
       "   'bnb_4bit_quant_type': 'nf4',\n",
       "   'bnb_4bit_use_double_quant': True,\n",
       "   'load_in_8bit': False},\n",
       "  'peft_config': {'r': 8,\n",
       "   'lora_alpha': 32,\n",
       "   'lora_dropout': 0.05,\n",
       "   'bias': 'none',\n",
       "   'task_type': 'SEQ_CLS',\n",
       "   'target_modules': 'all-linear'}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_config = {   # -----------------------\n",
    "                 'model_name': 'meta-llama/Meta-Llama-3-8B', # Pre-trained model names from the Hugging Face hub used for fine-tuning\n",
    "                 # --------------------------\n",
    "                  'split': (0.7, 0.3), # Divides the dataset into training, testing, (and optionally) validation sets. Examples: (0.7,0.3) -> split into train and test proportionally; (70, 30) splits into train,test proportionally.\n",
    "                 'binary': False, # Indicates whether the task is binary (two classes) or multi-class classification.,\n",
    "                 'balanced': (('train',), ('test',)),\n",
    "                 # --------------------------\n",
    "                 'training_arguments': {\n",
    "                     'num_train_epochs': 1, # Number of times the model sees the entire training dataset.\n",
    "                     'per_device_train_batch_size': 4, # Number of samples processed in each training step (personally, 8/16 work best, 16 is faster, but you may find linear drop in inference speed during fine-tuning).\n",
    "                     'per_device_eval_batch_size': 4, # Number of samples processed in each evaluation step.\n",
    "                     # 'gradient_accumulation_steps': 4,\n",
    "                     'gradient_checkpointing': True,\n",
    "                     #-----------------------------\n",
    "                     'save_total_limit': 2,\n",
    "                     'load_best_model_at_end': True,\n",
    "                     'save_strategy': 'steps', # Controls when to save model checkpoints ('steps', 'epoch' or 'no').\n",
    "                     'metric_for_best_model': 'f1-score',\n",
    "                     #-----------------------------\n",
    "                     'evaluation_strategy': \"steps\",\n",
    "                     'logging_steps': 100,\n",
    "                     'fp16': False,\n",
    "                     # 'use_cpu': False,\n",
    "                     #-----------------------------\n",
    "                     'learning_rate': 2e-5,\n",
    "                     'lr_scheduler_type': \"linear\",\n",
    "                     'warmup_ratio': 0.1,\n",
    "                     'max_grad_norm': 0.3,\n",
    "                     'weight_decay': 0.001,\n",
    "                 },\n",
    "                 #-----------------------------\n",
    "                     'bnb_config': [\n",
    "                                # False,\n",
    "                                {'bnb_4bit_compute_dtype': torch.bfloat16, 'load_in_4bit': True, 'bnb_4bit_quant_type': \"nf4\", 'bnb_4bit_use_double_quant': True, 'load_in_8bit': False}\n",
    "                                 ],\n",
    "                 'peft_config': [\n",
    "                                # False,\n",
    "                                {'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.05, 'bias': \"none\",\n",
    "                                'task_type': \"SEQ_CLS\", \n",
    "                                # 'target_modules': (\"v_proj\",),\n",
    "                                'target_modules': \"all-linear\"\n",
    "                                }\n",
    "                                ],\n",
    "                    }\n",
    "\n",
    "run_params_serie = prep_config_combinations(run_config)\n",
    "run_params_serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10da002dd892e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          text\n",
      "label         \n",
      "negative   604\n",
      "neutral   2879\n",
      "positive  1363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1008484cbb144a94b533761ffddea26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/4846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df143752da747b686ef59bf54e178fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0701ee5cc4a741c58fbaf74ea8c70476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078cde3eb09c46dba9a0ab4344336661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96324aae1da24b38a03b0c87c8748cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a311f4fab2d4462a799efd0d409b317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 21,016,600 || all params: 7,525,986,352 || trainable%: 0.2793\n",
      "pefted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: jakubpart (jpartyka). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\projects\\looptune\\notebooks\\wandb\\run-20240703_031253-krtwtm1d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jpartyka/huggingface/runs/krtwtm1d' target=\"_blank\">meta-llama/Meta-Llama-3-8B</a></strong> to <a href='https://wandb.ai/jpartyka/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jpartyka/huggingface' target=\"_blank\">https://wandb.ai/jpartyka/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jpartyka/huggingface/runs/krtwtm1d' target=\"_blank\">https://wandb.ai/jpartyka/huggingface/runs/krtwtm1d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 07:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.829600</td>\n",
       "      <td>1.210119</td>\n",
       "      <td>0.546508</td>\n",
       "      <td>0.508287</td>\n",
       "      <td>0.501747</td>\n",
       "      <td>0.508287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.884843</td>\n",
       "      <td>0.718395</td>\n",
       "      <td>0.686924</td>\n",
       "      <td>0.669811</td>\n",
       "      <td>0.686924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.641259</td>\n",
       "      <td>0.754789</td>\n",
       "      <td>0.751381</td>\n",
       "      <td>0.752562</td>\n",
       "      <td>0.751381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "D:\\Users\\Jakub\\anaconda3\\envs\\looptune\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.027 MB uploaded\\r'), FloatProgress(value=0.08941337069368108, max=1.‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñà</td></tr><tr><td>eval/f1-score</td><td>‚ñÅ‚ñÜ‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñá‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÜ‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÅ‚ñÜ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñà‚ñÉ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñà‚ñÉ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñá‚ñá‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÅ‚ñÉ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÉ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.75138</td></tr><tr><td>eval/f1-score</td><td>0.75256</td></tr><tr><td>eval/loss</td><td>0.64126</td></tr><tr><td>eval/precision</td><td>0.75479</td></tr><tr><td>eval/recall</td><td>0.75138</td></tr><tr><td>eval/runtime</td><td>46.7823</td></tr><tr><td>eval/samples_per_second</td><td>11.607</td></tr><tr><td>eval/steps_per_second</td><td>2.907</td></tr><tr><td>total_flos</td><td>2503516367694912.0</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>318</td></tr><tr><td>train/grad_norm</td><td>48.97945</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7524</td></tr><tr><td>train_loss</td><td>1.15957</td></tr><tr><td>train_runtime</td><td>464.6377</td></tr><tr><td>train_samples_per_second</td><td>2.731</td></tr><tr><td>train_steps_per_second</td><td>0.684</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">meta-llama/Meta-Llama-3-8B</strong> at: <a href='https://wandb.ai/jpartyka/huggingface/runs/krtwtm1d' target=\"_blank\">https://wandb.ai/jpartyka/huggingface/runs/krtwtm1d</a><br/> View project at: <a href='https://wandb.ai/jpartyka/huggingface' target=\"_blank\">https://wandb.ai/jpartyka/huggingface</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240703_031253-krtwtm1d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for run_params in run_params_serie:\n",
    "    single_run(run_params, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44167a32-d3f1-4aae-b355-d2f50649a9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
